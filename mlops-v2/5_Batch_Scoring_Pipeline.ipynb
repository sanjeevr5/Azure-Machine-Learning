{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f16148-cf9d-482e-bc14-6dfe3b051a7b",
   "metadata": {},
   "source": [
    "# Training an ML model with sweep and hosting a pipeline on batch endpoint\n",
    "\n",
    "Ref : https://github.com/Azure/azureml-examples/tree/main/sdk/python/endpoints/batch/deploy-pipelines/training-with-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c05c0d3-4974-40b9-af19-8c1bc41a4b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscriptionID = '2213e8b1-dbc7-4d54-8aff-b5e315df5e5b'\n",
    "RG = '1-44deb668-playground-sandbox'\n",
    "ws_name = \"MLOPS101\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f18074-3e04-438e-a40b-708b1119125b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x7f89c3a9f400>,\n",
      "         subscription_id=2213e8b1-dbc7-4d54-8aff-b5e315df5e5b,\n",
      "         resource_group_name=1-44deb668-playground-sandbox,\n",
      "         workspace_name=MLOPS101)\n"
     ]
    }
   ],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "ws = MLClient(DefaultAzureCredential(), subscription_id = subscriptionID,\n",
    "              resource_group_name = RG, workspace_name = ws_name)\n",
    "\n",
    "print(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9128959-ef53-4677-a5a7-c776af15265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path('./assets')\n",
    "ENV_DIR = ROOT_DIR / 'env'\n",
    "TRAIN_DIR = ROOT_DIR / 'train'\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "\n",
    "ENV_DIR.mkdir(parents = True, exist_ok = True)\n",
    "TRAIN_DIR.mkdir(parents = True, exist_ok = True)\n",
    "DATA_DIR.mkdir(parents = True, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92abe795-2d76-4e1d-9500-f402bedbdb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-06-10 14:31:14--  https://raw.githubusercontent.com/Azure/azureml-examples/main/sdk/python/endpoints/batch/deploy-pipelines/training-with-components/data/train/heart.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13273 (13K) [text/plain]\n",
      "Saving to: ‘assets/data/heart.csv’\n",
      "\n",
      "heart.csv           100%[===================>]  12.96K  --.-KB/s    in 0.008s  \n",
      "\n",
      "2023-06-10 14:31:14 (1.55 MB/s) - ‘assets/data/heart.csv’ saved [13273/13273]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/Azure/azureml-examples/main/sdk/python/endpoints/batch/deploy-pipelines/training-with-components/data/train/heart.csv -P assets/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7849792-c3bd-42b6-a0b6-daef1c9a1b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading data (0.01 MBs): 100%|██████████| 13273/13273 [00:00<00:00, 119980596.97it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with name heart_disease_train_data was registered to workspace, the dataset version is 1.0.0\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "heart_disease_train_url = \"\"\n",
    "\n",
    "heart_disease_train_data = Data(\n",
    "    name = \"heart_disease_train_data\",\n",
    "    path = str(DATA_DIR),\n",
    "    type = AssetTypes.URI_FOLDER,\n",
    "    description = \"Train heart disease dataset\",\n",
    "    tags = {\"source_type\": \"web\"},\n",
    "    version = \"1.0.0\",\n",
    ")\n",
    "\n",
    "heart_disease_train_data = ws.data.create_or_update(heart_disease_train_data)\n",
    "print(f\"Dataset with name {heart_disease_train_data.name} was registered to workspace, the dataset version is {heart_disease_train_data.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c33743bf-c1ab-46a3-bc0e-9986f2866208",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease_train_data = ws.data.get(name = 'heart_disease_train_data', label=\"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f92b0a1-e40c-43ac-96ff-64b85abb22b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_Resource__source_path',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_auto_increment_version',\n",
       " '_base_path',\n",
       " '_creation_context',\n",
       " '_from_container_rest_object',\n",
       " '_from_rest_object',\n",
       " '_get_arm_resource',\n",
       " '_get_arm_resource_and_params',\n",
       " '_id',\n",
       " '_is_anonymous',\n",
       " '_load',\n",
       " '_load_from_dict',\n",
       " '_mltable_schema_url',\n",
       " '_path',\n",
       " '_referenced_uris',\n",
       " '_resolve_cls_and_type',\n",
       " '_serialize',\n",
       " '_skip_validation',\n",
       " '_source_path',\n",
       " '_to_container_rest_object',\n",
       " '_to_dict',\n",
       " '_to_rest_object',\n",
       " '_update_path',\n",
       " '_version',\n",
       " 'base_path',\n",
       " 'creation_context',\n",
       " 'datastore',\n",
       " 'description',\n",
       " 'dump',\n",
       " 'id',\n",
       " 'latest_version',\n",
       " 'name',\n",
       " 'path',\n",
       " 'print_as_yaml',\n",
       " 'properties',\n",
       " 'tags',\n",
       " 'type',\n",
       " 'version']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(heart_disease_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a07ba7d-5213-449c-8e0a-6a58e3f5e29a",
   "metadata": {},
   "source": [
    "## Creating a conda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f02e970-70ce-4af0-bb5a-e6154d62c915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing assets/env/conda_definition.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {ENV_DIR}/conda_definition.yml\n",
    "\n",
    "name: heart-env\n",
    "channels:\n",
    "- conda-forge\n",
    "dependencies:\n",
    "- python=3.8.5\n",
    "- pip\n",
    "- pip:\n",
    "  - mlflow\n",
    "  - azureml-mlflow\n",
    "  - datasets\n",
    "  - jobtools\n",
    "  - cloudpickle==1.6.0\n",
    "  - dask==2.30.0\n",
    "  - scikit-learn==1.1.2\n",
    "  - xgboost==1.3.3\n",
    "  - pandas==1.4\n",
    "  - matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c2dc8fd-f20a-4a4e-ab33-f093def86b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment with name heart-env is registered to workspace, the environment version is 0.1.0\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "custom_env_name = \"heart-env\"\n",
    "\n",
    "heart_env = Environment(\n",
    "    name = custom_env_name,\n",
    "    description = \"Custom environment for heart disease classification\",\n",
    "    #tags={\"scikit-learn\": \"0.24.2\"},\n",
    "    conda_file = f'{ENV_DIR}/conda_definition.yml',\n",
    "    image = \"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    "    version=\"0.1.0\",\n",
    ")\n",
    "heart_env = ws.environments.create_or_update(heart_env)\n",
    "\n",
    "print(\n",
    "   f\"Environment with name {heart_env.name} is registered to workspace, the environment version is {heart_env.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544b6f7-d5b9-4dfa-b988-facd6e51c1d2",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fb51b11-a65e-46e3-95ac-a2d912ad0e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing assets/train/process.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAIN_DIR}/process.yml\n",
    "\n",
    "name: heart_disease_prepare\n",
    "display_name : heart disease data preparation component\n",
    "type: command\n",
    "inputs:\n",
    "    raw_data:\n",
    "        type: uri_folder\n",
    "    transformations_input:\n",
    "        type: custom_model\n",
    "        optional: true\n",
    "    categorical_encoding:\n",
    "        type: string\n",
    "        optional: true\n",
    "        default: ordinal\n",
    "outputs:\n",
    "    prepared_data:\n",
    "        type: uri_folder\n",
    "    transformations_output:\n",
    "        type: custom_model\n",
    "code: ./process.py\n",
    "environment: azureml:heart-env@latest\n",
    "command: >-\n",
    "    python process.py\n",
    "    --raw_data ${{inputs.raw_data}}\n",
    "    $[[--transformations_input ${{inputs.transformations_input}}]]\n",
    "    $[[--categorical_encoding ${{inputs.categorical_encoding}}]]\n",
    "    --prepared_data ${{outputs.prepared_data}}\n",
    "    --transformations_output ${{outputs.transformations_output}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34487adf-5d49-45dc-b1c2-c177fe7b1602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing assets/train/process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAIN_DIR}/process.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import joblib\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import mlflow\n",
    "\n",
    "transform_filename = 'column_transformer.pkl'\n",
    "continuous_features = ['age', 'chol', 'oldpeak', 'thalach', 'trestbps']\n",
    "discrete_features = ['ca', 'cp', 'exang', 'fbs', 'restecg', 'sex', 'slope', 'thal']\n",
    "target_column = 'target'\n",
    "\n",
    "\n",
    "def preprocessing_pipeline(categorical_encoding, cf, df): #cf -> Continuous feat df -> discrete feat\n",
    "    try:\n",
    "        if categorical_encoding == 'ordinal':\n",
    "            cat_enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)\n",
    "        elif categorical_encoding == 'onehot':\n",
    "            cat_enc = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "        else:\n",
    "            raise NotImplementedError('Possible values are ordinal or onehot')\n",
    "\n",
    "        conti_feat_pipeline = sklearn.pipeline.Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy = 'median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        disc_feat_pipeline = sklearn.pipeline.Pipeline([\n",
    "\n",
    "            ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "            ('encoder', cat_enc)\n",
    "\n",
    "        ])\n",
    "\n",
    "        transformations = ColumnTransformer([\n",
    "            ('conti_feat_pipeline', conti_feat_pipeline, cf),\n",
    "            ('disc_feat_pipeline', disc_feat_pipeline, df)\n",
    "        ])\n",
    "        return transformations\n",
    "    except Exception as e:\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(exc_type, fname, exc_tb.tb_lineno, e)\n",
    "\n",
    "def preprocess_data(dataframe, label, cf, df, cat_enc = 'ordinal', transformations = None):\n",
    "    \n",
    "    mlflow.sklearn.autolog()\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        if label in dataframe.columns:\n",
    "            X = dataframe.iloc[:,:-1]\n",
    "            restore_target = True\n",
    "        else:\n",
    "            X = dataframe\n",
    "            restore_target = False\n",
    "\n",
    "        if transformations:\n",
    "            X_transformed = transformations.transform(X)\n",
    "        else:\n",
    "            transformations = preprocessing_pipeline(cat_enc, cf, df)\n",
    "            X_transformed = transformations.fit_transform(X)\n",
    "        transformed_discrete_features = (\n",
    "            transformations.transformers_[1][1]\n",
    "            .named_steps[\"encoder\"]\n",
    "            .get_feature_names_out(discrete_features)\n",
    "        )\n",
    "        all_features = continuous_features + list(transformed_discrete_features)\n",
    "\n",
    "        if restore_target:\n",
    "            target_values = dataframe[label].to_numpy().reshape(len(dataframe), 1)\n",
    "            X_transformed = np.hstack((X_transformed, target_values))\n",
    "            all_features.append(label)\n",
    "\n",
    "        return pd.DataFrame(X_transformed, columns = all_features), transformations\n",
    "    \n",
    "    except Exception as e:\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(exc_type, fname, exc_tb.tb_lineno, e)\n",
    "        \n",
    "def parseArgs():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--raw_data', type = str)\n",
    "    parser.add_argument('--categorical_encoding', type = str, required = False)\n",
    "    parser.add_argument('--transformations_input', type = str, required = False)\n",
    "    parser.add_argument('--prepared_data', type = str)\n",
    "    parser.add_argument('--transformations_output', type = str)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    return args\n",
    "\n",
    "def main(args):\n",
    "    try:\n",
    "        if args.transformations_input:\n",
    "            transformations_input = str(Path(args.transformations_input) / transform_filename)\n",
    "            if os.path.exists(transformations_input):\n",
    "                transformations = joblib.load(args.transformations_input)\n",
    "            else:\n",
    "                transformations = None\n",
    "        else:\n",
    "            transformations = None\n",
    "        print('[Debug] Transformations input defined successfully')\n",
    "        files = list(Path(args.raw_data).rglob('*.csv'))\n",
    "\n",
    "        with mlflow.start_run(nested=True):\n",
    "            for file in files:\n",
    "                print(f'Working with file {file}')\n",
    "                temp = pd.read_csv(file)\n",
    "                preprocessed, transformations = preprocess_data(\n",
    "                    temp,\n",
    "                    target_column,\n",
    "                    continuous_features,\n",
    "                    discrete_features,\n",
    "                    args.categorical_encoding,\n",
    "                    transformations\n",
    "                )\n",
    "                output_file_name = Path(file).stem\n",
    "                output_file_path = str(Path(args.prepared_data) / f'{output_file_name}.csv')\n",
    "                print(f'Writing file {output_file_path}')\n",
    "                preprocessed.to_csv(output_file_path, index=False)\n",
    "        transformations_output_path = str(Path(args.transformations_output) / transform_filename)\n",
    "        joblib.dump(transformations, transformations_output_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(exc_type, fname, exc_tb.tb_lineno, e)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parseArgs()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a033e8-1486-461c-b226-ecfc62349196",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "371efa0e-0284-432b-a512-c4e794283723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing assets/train/train.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAIN_DIR}/train.yml\n",
    "\n",
    "name: heart_disease_train\n",
    "display_name: Training definition - heart disease dataset\n",
    "type: command\n",
    "inputs:\n",
    "    data:\n",
    "        type: uri_folder\n",
    "    target_column:\n",
    "        type: string\n",
    "    test_size:\n",
    "        type: number\n",
    "        default: 0.3\n",
    "    register_model:\n",
    "        type: boolean\n",
    "        default: true\n",
    "    model_name:\n",
    "        type: string\n",
    "        default: best_model\n",
    "outputs:\n",
    "    model_folder:\n",
    "        type: mlflow_model\n",
    "    results_folder:\n",
    "        type: uri_folder\n",
    "environment: azureml:heart-env@latest\n",
    "code: ./train.py\n",
    "command: >-\n",
    "    python train.py\n",
    "    --data ${{inputs.data}}\n",
    "    --target_column ${{inputs.target_column}}\n",
    "    --test_size ${{inputs.test_size}}\n",
    "    --register_model ${{inputs.register_model}}\n",
    "    --model_name ${{inputs.model_name}}\n",
    "    --model_folder ${{outputs.model_folder}}\n",
    "    --results_folder ${{outputs.results_folder}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cf84825-c982-4200-89a0-c5f5085e1198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting assets/train/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAIN_DIR}/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from distutils.util import strtobool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from xgboost import XGBClassifier\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\"score\")\n",
    "parser.add_argument('--data', type=str)\n",
    "parser.add_argument('--target_column', type=str)\n",
    "parser.add_argument('--test_size', type=float, default = 0.3)#, required=False)\n",
    "parser.add_argument(\"--register_model\", type=lambda x: bool(strtobool(x)))\n",
    "parser.add_argument('--model_name', type=str, default = 'best_model')\n",
    "parser.add_argument('--model_folder', type=str)\n",
    "parser.add_argument('--results_folder', type=str)\n",
    "args = parser.parse_args()\n",
    "print(vars(args))\n",
    "\n",
    "try:\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.xgboost.autolog(log_models=False)\n",
    "        input_files = list(Path(args.data).rglob('*.csv'))\n",
    "        print(input_files)\n",
    "        print(pd.read_csv(input_files[0]))\n",
    "        df = pd.concat(map(pd.read_csv, input_files))\n",
    "        if args.test_size > 0:\n",
    "            train, test = train_test_split(df, test_size = args.test_size)\n",
    "        else:\n",
    "            train = df\n",
    "            test = df\n",
    "        train_features = train.drop(columns=[args.target_column])\n",
    "        train_target = train[args.target_column]\n",
    "\n",
    "        model = XGBClassifier(scale_pos_weight=99)\n",
    "        model.fit(train_features, train_target)\n",
    "\n",
    "        test_features = test.drop(columns=[args.target_column])\n",
    "        predictions = model.predict(test_features)\n",
    "        test['Labels'] = predictions\n",
    "        test['Probabilities'] = model.predict_proba(test_features)[:, 1]\n",
    "        test.to_csv(\n",
    "            os.path.join(args.results_folder, 'test_predictions.csv'), index=False\n",
    "        )\n",
    "\n",
    "        accuracy = accuracy_score(test[args.target_column], predictions)\n",
    "        recall = recall_score(test[args.target_column], predictions)\n",
    "        mlflow.log_metrics({'accuracy': accuracy, 'recall': recall})\n",
    "\n",
    "        # Model logging\n",
    "        signature = infer_signature(train_features, predictions)\n",
    "        mlflow.xgboost.save_model(model, args.model_folder, signature=signature)\n",
    "\n",
    "        if args.register_model:\n",
    "            mlflow.xgboost.log_model(\n",
    "                model,\n",
    "                'model',\n",
    "                signature=signature,\n",
    "                registered_model_name = args.registered_model_name,\n",
    "            )\n",
    "        else:\n",
    "            mlflow.xgboost.log_model(model, 'model', signature=signature)\n",
    "except Exception as e:\n",
    "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "    print(exc_type, fname, exc_tb.tb_lineno, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc290ac-ee9e-4016-a633-6fa33f513a8c",
   "metadata": {},
   "source": [
    "## Creating cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf298aef-adac-4088-88f0-ce738d8fc8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new cpu compute target...\n",
      "Cluster created successfully named iris-cluster with size STANDARD_D2_V2\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "\n",
    "def createCluster(cluster_name, size):\n",
    "    try:\n",
    "        cpu_cluster = ws.compute.get(cluster_name)\n",
    "        print(f'{CLUSTER_NAME} exists!')\n",
    "    except Exception:\n",
    "        print(\"Creating a new cpu compute target...\")\n",
    "        cpu_cluster = AmlCompute(\n",
    "            name=cluster_name,\n",
    "            type=\"amlcompute\",\n",
    "            size=size,\n",
    "            min_instances=0,\n",
    "            max_instances=1,\n",
    "            idle_time_before_scale_down=3000,\n",
    "            tier=\"Dedicated\",\n",
    "        )\n",
    "        cpu_cluster = ws.compute.begin_create_or_update(cpu_cluster).result()\n",
    "        print(f'Cluster created successfully named {cpu_cluster.name} with size {cpu_cluster.size}')\n",
    "    return cpu_cluster\n",
    "\n",
    "CLUSTER_NAME = 'iris-cluster'\n",
    "CLUSTER_SIZE = 'Standard_D2_v2'\n",
    "\n",
    "trainCluster = createCluster(CLUSTER_NAME, CLUSTER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a818ace-e97f-4ca4-a89a-c6a0a24b27a2",
   "metadata": {},
   "source": [
    "## Pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "850e4aae-c3a5-4572-8838-6c77cfefe1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import dsl, Input, Output, load_component\n",
    "\n",
    "data_preparation = load_component(source = f'{TRAIN_DIR}/process.yml')\n",
    "train = load_component(source = f'{TRAIN_DIR}/train.yml')\n",
    "\n",
    "@dsl.pipeline(compute = trainCluster, description = 'Batch pipeline prediction')\n",
    "def uci_heart_classifier_trainer(input_data):\n",
    "    prepared_data = data_preparation(raw_data=input_data)\n",
    "    trained_model = train(\n",
    "        data = prepared_data.outputs.prepared_data,\n",
    "        target_column = 'target',\n",
    "        register_model = True,\n",
    "        test_size = 0.3\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'model': trained_model.outputs.model_folder,\n",
    "        'evaluation_results': trained_model.outputs.results_folder,\n",
    "        'transformations_output': prepared_data.outputs.transformations_output,\n",
    "    }\n",
    "\n",
    "pipeline_job = uci_heart_classifier_trainer(\n",
    "    Input(type = 'uri_folder', path = heart_disease_train_data.id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c643ae5a-0409-470b-89b3-b0bfd6fbe190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading train.py\u001b[32m (< 1 MB): 100%|██████████| 2.72k/2.72k [00:00<00:00, 104kB/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>uci-heart-train-pipeline</td><td>upbeat_sugar_8x13zyd6f6</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/upbeat_sugar_8x13zyd6f6?wsid=/subscriptions/2213e8b1-dbc7-4d54-8aff-b5e315df5e5b/resourcegroups/1-44deb668-playground-sandbox/workspaces/MLOPS101&amp;tid=84f1e4ea-8554-43e1-8709-f0b8589ea118\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {'input_data': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f8992ef7400>}, 'outputs': {'model': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f8992ef74c0>, 'evaluation_results': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f8992ef7490>, 'transformations_output': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f8992ef74f0>}, 'jobs': {}, 'component': PipelineComponent({'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'name': 'azureml_anonymous', 'description': 'Batch pipeline prediction', 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/mlops101/code/Users/cloud_user_p_af8bc55f', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f8992ef6fe0>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'uci_heart_classifier_trainer', 'is_deterministic': None, 'inputs': {'input_data': {}}, 'outputs': {'model': {}, 'evaluation_results': {}, 'transformations_output': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'prepared_data': Command({'parameters': {}, 'init': False, 'type': 'command', 'status': None, 'log_files': None, 'name': 'prepared_data', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/mlops101/code/Users/cloud_user_p_af8bc55f', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f8992ef6ce0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'raw_data': '${{parent.inputs.input_data}}'}, 'job_outputs': {'transformations_output': '${{parent.outputs.transformations_output}}'}, 'inputs': {'raw_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f8992ef6d40>}, 'outputs': {'transformations_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f8992ef70a0>}, 'component': 'azureml_anonymous:914764af-22a0-4fd1-b7e9-54bc0f1e6028', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'e290aecd-eb7d-4189-a11e-06366338d2ce', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False}), 'trained_model': Command({'parameters': {}, 'init': False, 'type': 'command', 'status': None, 'log_files': None, 'name': 'trained_model', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/mlops101/code/Users/cloud_user_p_af8bc55f', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f8992ef6da0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'target_column': 'target', 'test_size': '0.3', 'register_model': 'True', 'data': '${{parent.jobs.prepared_data.outputs.prepared_data}}'}, 'job_outputs': {'model_folder': '${{parent.outputs.model}}', 'results_folder': '${{parent.outputs.evaluation_results}}'}, 'inputs': {'target_column': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f8992ef4100>, 'test_size': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f8992ef6f20>, 'register_model': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f8992ef6f50>, 'data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f8992ef6ec0>}, 'outputs': {'model_folder': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f8992ef6bc0>, 'results_folder': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f8992ef6e90>}, 'component': 'azureml_anonymous:165e6c5c-41d0-4dee-9708-1d18fdfa335a', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'b6e5783c-18f2-4348-88a8-b1d3959f26c0', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 2}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 2}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'upbeat_sugar_8x13zyd6f6', 'description': 'Batch pipeline prediction', 'tags': {}, 'properties': {'azureml.DevPlatv2': 'true', 'azureml.DatasetAccessMode': 'Asset', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'True', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.enforceRerun': 'False', 'azureml.defaultComputeName': 'iris-cluster', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'print_as_yaml': True, 'id': '/subscriptions/2213e8b1-dbc7-4d54-8aff-b5e315df5e5b/resourceGroups/1-44deb668-playground-sandbox/providers/Microsoft.MachineLearningServices/workspaces/MLOPS101/jobs/upbeat_sugar_8x13zyd6f6', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/mlops101/code/Users/cloud_user_p_af8bc55f', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f8992ef7370>, 'serialize': <msrest.serialization.Serializer object at 0x7f8992ef7520>, 'display_name': 'uci_heart_classifier_trainer', 'experiment_name': 'uci-heart-train-pipeline', 'compute': 'iris-cluster', 'services': {'Tracking': <azure.ai.ml.entities._job.job_service.JobService object at 0x7f8992ef73a0>, 'Studio': <azure.ai.ml.entities._job.job_service.JobService object at 0x7f8992ef7460>}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_job_run = ws.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"uci-heart-train-pipeline\"\n",
    ")\n",
    "pipeline_job_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af1c841b-3ecd-4357-b156-695eda679629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: upbeat_sugar_8x13zyd6f6\n",
      "Web View: https://ml.azure.com/runs/upbeat_sugar_8x13zyd6f6?wsid=/subscriptions/2213e8b1-dbc7-4d54-8aff-b5e315df5e5b/resourcegroups/1-44deb668-playground-sandbox/workspaces/MLOPS101\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: upbeat_sugar_8x13zyd6f6\n",
      "Web View: https://ml.azure.com/runs/upbeat_sugar_8x13zyd6f6?wsid=/subscriptions/2213e8b1-dbc7-4d54-8aff-b5e315df5e5b/resourcegroups/1-44deb668-playground-sandbox/workspaces/MLOPS101\n"
     ]
    },
    {
     "ename": "JobException",
     "evalue": "Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /trained_model. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"eastus2\",\n    \"location\": \"eastus2\",\n    \"time\": \"2023-06-10T15:04:58.546618Z\",\n    \"component_name\": \"\"\n} ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJobException\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mws\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_job_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/tracing/decorator.py:76\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:263\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_operations.py:645\u001b[0m, in \u001b[0;36mJobOperations.stream\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pipeline_child_job(job_object):\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineChildJobError(job_id\u001b[38;5;241m=\u001b[39mjob_object\u001b[38;5;241m.\u001b[39mid)\n\u001b[0;32m--> 645\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_logs_until_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runs_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datastore_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequests_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requests_pipeline\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_ops_helper.py:295\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[0;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[1;32m    293\u001b[0m         file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m JobException(\n\u001b[1;32m    296\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException : \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(json\u001b[38;5;241m.\u001b[39mdumps(error, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)),\n\u001b[1;32m    297\u001b[0m             target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mJOB,\n\u001b[1;32m    298\u001b[0m             no_personal_data_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException raised on failed job.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    299\u001b[0m             error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mSYSTEM_ERROR,\n\u001b[1;32m    300\u001b[0m         )\n\u001b[1;32m    302\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    303\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mJobException\u001b[0m: Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /trained_model. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"eastus2\",\n    \"location\": \"eastus2\",\n    \"time\": \"2023-06-10T15:04:58.546618Z\",\n    \"component_name\": \"\"\n} "
     ]
    }
   ],
   "source": [
    "ws.jobs.stream(pipeline_job_run.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347cd9a7-9ee5-41c6-ab40-3edee1a2cfb3",
   "metadata": {},
   "source": [
    "https://github.com/Azure/azureml-examples/blob/main/sdk/python/endpoints/batch/deploy-pipelines/training-with-components/sdk-deploy-and-test.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
