{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d9396b-dc6b-4d13-9458-01842b71c915",
   "metadata": {},
   "source": [
    "# E2E ML Pipeline With AML\n",
    "\n",
    "- A simple classification model with E2E ML Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7da0cdd-caed-4e71-b985-553bd3428f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscriptionID = '2213e8b1-dbc7-4d54-8aff-b5e315df5e5b'\n",
    "RG = '1-6a7b0882-playground-sandbox'\n",
    "ws_name = \"MLOPS101\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193f7c21-2d53-4b28-b48b-2f6456d14b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x7f7d477d82b0>,\n",
      "         subscription_id=2213e8b1-dbc7-4d54-8aff-b5e315df5e5b,\n",
      "         resource_group_name=1-6a7b0882-playground-sandbox,\n",
      "         workspace_name=MLOPS101)\n"
     ]
    }
   ],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "ws = MLClient(\n",
    "    DefaultAzureCredential(),\n",
    "    subscription_id = subscriptionID,\n",
    "    resource_group_name = RG,\n",
    "    workspace_name= ws_name,\n",
    ")\n",
    "print(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fde7969-6c97-4750-9618-809ca2f571b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with name raw_iris_data was registered to workspace, the dataset version is 1.0.0\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "iris_url = \"https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv\"\n",
    "\n",
    "iris_data = Data(\n",
    "    name = \"raw_iris_data\",\n",
    "    path = iris_url,\n",
    "    type = AssetTypes.URI_FILE,\n",
    "    description = \"Uncleansed IRIS dataset\",\n",
    "    tags = {\"source_type\": \"web\"},\n",
    "    version = \"1.0.0\",\n",
    ") #Stores in the default datastore\n",
    "\n",
    "iris_data = ws.data.create_or_update(iris_data)\n",
    "print(f\"Dataset with name {iris_data.name} was registered to workspace, the dataset version is {iris_data.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e36dec77-fe80-49da-b1fb-5a21d468fee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new cpu compute target...\n",
      "Cluster created successfully named iris-cluster with size STANDARD_D2_V2\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "\n",
    "def createCluster(cluster_name, size):\n",
    "    try:\n",
    "        cpu_cluster = ws.compute.get(cluster_name)\n",
    "        print(f'{CLUSTER_NAME} exists!')\n",
    "    except Exception:\n",
    "        print(\"Creating a new cpu compute target...\")\n",
    "        cpu_cluster = AmlCompute(\n",
    "            name=cluster_name,\n",
    "            type=\"amlcompute\",\n",
    "            size=size,\n",
    "            min_instances=0,\n",
    "            max_instances=1,\n",
    "            idle_time_before_scale_down=180,\n",
    "            tier=\"Dedicated\",\n",
    "        )\n",
    "        cpu_cluster = ws.compute.begin_create_or_update(cpu_cluster).result()\n",
    "        print(f'Cluster created successfully named {cpu_cluster.name} with size {cpu_cluster.size}')\n",
    "    return cpu_cluster\n",
    "\n",
    "CLUSTER_NAME = 'iris-cluster'\n",
    "CLUSTER_SIZE = 'Standard_D2_v2'\n",
    "\n",
    "trainCluster = createCluster(CLUSTER_NAME, CLUSTER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5c15c1-e496-4ca6-aa14-1568533ed195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PARENT_DIR = Path(\"./E2EPipelines\")\n",
    "ENVIRONMENT_DIR = PARENT_DIR / \"environment\"\n",
    "DATA_PREP_DIR = PARENT_DIR / \"data_prep\"\n",
    "TRAIN_DIR = PARENT_DIR / \"train\"\n",
    "\n",
    "ENVIRONMENT_DIR.mkdir(parents = True, exist_ok = True)\n",
    "DATA_PREP_DIR.mkdir(parents = True, exist_ok = True)\n",
    "TRAIN_DIR.mkdir(parents = True, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f32b1b0-1755-4627-bf63-dd3cd8b2198d",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Refs:\n",
    "\n",
    "1. https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-component-pipeline-python?view=azureml-api-2\n",
    "2. https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-sweep-in-pipeline?view=azureml-api-2\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccd8c25d-f168-4645-825e-637ed997499c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing E2EPipelines/environment/conda.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {ENVIRONMENT_DIR}/conda.yaml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - numpy=1.21.2\n",
    "  - pip=21.2.4\n",
    "  - scikit-learn=0.24.2\n",
    "  - scipy=1.7.1\n",
    "  - pandas>=1.1,<1.2\n",
    "  - pip:\n",
    "    - inference-schema[numpy-support]==1.3.0\n",
    "    - xlrd==2.0.1\n",
    "    - mlflow== 1.26.1\n",
    "    - azureml-mlflow==1.42.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a745814-cd23-46f6-a7da-a73683eebd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment with name iris-env is registered to workspace, the environment version is 0.1.0\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "custom_env_name = \"iris-env\"\n",
    "\n",
    "iris_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for IRIS data\",\n",
    "    tags={\"scikit-learn\": \"0.24.2\"},\n",
    "    conda_file = 'E2EPipelines/environment/conda.yaml',\n",
    "    image = \"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    "    version=\"0.1.0\",\n",
    ")\n",
    "iris_env = ws.environments.create_or_update(iris_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {iris_env.name} is registered to workspace, the environment version is {iris_env.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378baf1d-ac92-4eb3-ace4-dbca92ae6538",
   "metadata": {},
   "source": [
    "## Data Preparation Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8d9a98f-9c68-4221-9ff1-fcf990d051ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing E2EPipelines/data_prep/data_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {DATA_PREP_DIR}/data_prep.py\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import mlflow\n",
    "\n",
    "def labelEncoder(df):\n",
    "    le = LabelEncoder().fit(df.variety)\n",
    "    df.variety = le.transform(df.variety)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--raw_data\", type = str, help = \"Datastore path to the input data\")\n",
    "    parser.add_argument(\"--split_ratio\", type=float, required=False, default=0.25)\n",
    "    parser.add_argument(\"--train_data_dir\", type = str, help = \"Path to the data processed directory\")\n",
    "    parser.add_argument(\"--test_data_dir\", type = str, help = \"Path to the data processed directory\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    mlflow.start_run()\n",
    "\n",
    "    mlflow.log_param(\"Input ARGS\", \" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
    "    mlflow.log_param(\"input data:\", args.raw_data)\n",
    "\n",
    "    iris_df = pd.read_csv(args.raw_data)\n",
    "    mlflow.log_param(\"num_samples\", iris_df.shape[0])\n",
    "    mlflow.log_param(\"num_features\", iris_df.shape[1] - 1)\n",
    "    \n",
    "    iris_df = labelEncoder(iris_df)\n",
    "    iris_train_df, iris_test_df = train_test_split(\n",
    "        iris_df,\n",
    "        test_size=args.split_ratio,\n",
    "        stratify = iris_df.variety\n",
    "    )\n",
    "    mlflow.log_param(\"train_path\", args.train_data_dir)\n",
    "    mlflow.log_param(\"test_path\", args.test_data_dir)\n",
    "    iris_train_df.to_csv(os.path.join(args.train_data_dir, \"data.csv\"), index=False)\n",
    "    iris_test_df.to_csv(os.path.join(args.test_data_dir, \"data.csv\"), index=False)\n",
    "    mlflow.end_run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "929e470a-5bef-4093-8475-5a6e25e9b878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing E2EPipelines/data_prep/data_prep.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {DATA_PREP_DIR}/data_prep.yml\n",
    "\n",
    "name: iris_data_prep\n",
    "display_name: IRIS Data Prep\n",
    "type: command\n",
    "inputs:\n",
    "    raw_data: \n",
    "        type: uri_file\n",
    "    split_ratio:\n",
    "        type: number\n",
    "outputs:\n",
    "    train_data_dir:\n",
    "        type: uri_folder\n",
    "        mode: rw_mount\n",
    "    test_data_dir:\n",
    "        type: uri_folder\n",
    "        mode: rw_mount\n",
    "code:\n",
    "    ./data_prep.py\n",
    "environment:\n",
    "    azureml:iris-env:0.1.0\n",
    "command: >-\n",
    "    python data_prep.py\n",
    "    --raw_data  ${{inputs.raw_data}}\n",
    "    --split_ratio ${{inputs.split_ratio}}\n",
    "    --train_data ${{outputs.train_data_dir}}\n",
    "    --test_data ${{outputs.test_data_dir}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8920e361-4983-421a-a808-87f15d548ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading data_prep.py\u001b[32m (< 1 MB): 100%|██████████| 1.63k/1.63k [00:00<00:00, 126kB/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component iris_data_prep with Version 1 is registered\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import load_component\n",
    "\n",
    "iris_data_prep_component = load_component(source = DATA_PREP_DIR/ 'data_prep.yml')\n",
    "\n",
    "iris_data_prep_component = ws.create_or_update(iris_data_prep_component)\n",
    "\n",
    "print(f\"Component {iris_data_prep_component.name} with Version {iris_data_prep_component.version} is registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b72c78-634b-4dfa-aab3-41d6122200ac",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b676cc23-4d3a-4830-bc81-2ca8e167c443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting E2EPipelines/train/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAIN_DIR}/train.py\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def select_first_file(path):\n",
    "    files = os.listdir(path)\n",
    "    return os.path.join(path, files[0])\n",
    "\n",
    "mlflow.start_run()\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "OUTPUT_DIR = Path('./outputs')\n",
    "OUTPUT_DIR.mkdir(exist_ok = True)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train_data_dir\", type=str, help=\"path to train data\")\n",
    "    parser.add_argument(\"--test_data_dir\", type=str, help=\"path to test data\")\n",
    "    parser.add_argument(\"--model_name\", type=str, help=\"path to model file\")\n",
    "    parser.add_argument(\"--model_dir\", type=str, help=\"path to model file\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    train_df = pd.read_csv(select_first_file(args.train_data_dir))\n",
    "    y_train, X_train = train_df.pop(\"variety\"), train_df.values\n",
    "    \n",
    "    test_df = pd.read_csv(select_first_file(args.test_data_dir))\n",
    "    y_test, X_test = test_df.pop(\"variety\"), test_df.values\n",
    "\n",
    "    mlflow.log_param('Train data shape', X_train.shape)\n",
    "    mlflow.log_param('Test data shape', X_test.shape)\n",
    "\n",
    "    clf = GradientBoostingClassifier(\n",
    "        n_estimators =  100, learning_rate = 0.01\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Registering the model to the workspace\n",
    "    print(\"Registering the model via MLFlow\")\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=clf,\n",
    "        registered_model_name = args.model_name,\n",
    "        artifact_path = args.model_name,\n",
    "    )\n",
    "    mlflow.sklearn.save_model(\n",
    "        sk_model=clf,\n",
    "        path=os.path.join(args.model_dir, \"trained_model\"),\n",
    "    )\n",
    "    mlflow.end_run()\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e912f2f-c239-4a82-9b82-6589cf553064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting E2EPipelines/train/train.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAIN_DIR}/train.yml\n",
    "\n",
    "name: iris_train\n",
    "display_name: IRIS Train Step\n",
    "type: command\n",
    "inputs:\n",
    "    train_data_dir: \n",
    "        type: uri_folder\n",
    "    test_data_dir:\n",
    "        type: uri_folder\n",
    "    model_name:\n",
    "        type: string\n",
    "outputs:\n",
    "    model_dir:\n",
    "        type: uri_folder\n",
    "code: ./train.py\n",
    "environment: azureml:iris-env:0.1.0\n",
    "command: >-\n",
    "    python train.py\n",
    "    --train_data_dir  ${{inputs.train_data_dir}}\n",
    "    --test_data_dir ${{inputs.test_data_dir}}\n",
    "    --model_name ${{inputs.model_name}}\n",
    "    --model_dir ${{outputs.model_dir}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a48106c0-e011-497f-b71e-08a7afbe3b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading train.py\u001b[32m (< 1 MB): 100%|██████████| 1.75k/1.75k [00:00<00:00, 117kB/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component iris_train with Version 2023-05-30-14-15-26-7758220 is registered\n"
     ]
    }
   ],
   "source": [
    "iris_train_component = load_component(source = TRAIN_DIR/ 'train.yml')\n",
    "\n",
    "iris_train_component = ws.create_or_update(iris_train_component)\n",
    "\n",
    "print(f\"Component {iris_train_component.name} with Version {iris_train_component.version} is registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc99ce-1514-47c5-96ef-7832df4e87f5",
   "metadata": {},
   "source": [
    "## Building a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adef4104-a10d-4e14-9d61-5c33377aca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import dsl, Input, Output\n",
    "\n",
    "\n",
    "@dsl.pipeline(compute = trainCluster, description = \"E2E IRIS pipeline\")\n",
    "def iris_pipeline(raw_data, split_ratio, model_name):\n",
    "    data_prep_job = iris_data_prep_component(raw_data = raw_data, split_ratio = split_ratio)\n",
    "    train_data_dir, test_data_dir = data_prep_job.outputs.train_data_dir, data_prep_job.outputs.test_data_dir\n",
    "    train_job = iris_train_component(train_data_dir = train_data_dir,test_data_dir = test_data_dir, model_name = model_name)\n",
    "    return {\n",
    "        \"pipeline_job_train_data\": train_data_dir,\n",
    "        \"pipeline_job_test_data\": test_data_dir,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8171f158-40c5-4ed8-9f35-7c0e4796a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"iris_model\"\n",
    "\n",
    "# Let's instantiate the pipeline with the parameters of our choice\n",
    "pipeline = iris_pipeline(\n",
    "    raw_data=Input(type=\"uri_file\", path = iris_data.path),\n",
    "    split_ratio=0.25,\n",
    "    model_name=\"iris_model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cceac11-aae8-4051-bef3-966b0880312a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "# submit the pipeline job\n",
    "pipeline_job = ws.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    experiment_name=\"e2e_registered_components\",\n",
    ")\n",
    "webbrowser.open(pipeline_job.studio_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3eec4864-2905-4a85-90d9-83e06d8f8d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: yellow_wolf_bvfprd58p6\n",
      "Web View: https://ml.azure.com/runs/yellow_wolf_bvfprd58p6?wsid=/subscriptions/2213e8b1-dbc7-4d54-8aff-b5e315df5e5b/resourcegroups/1-6a7b0882-playground-sandbox/workspaces/MLOPS101\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2023-05-30 14:15:32Z] Completing processing run id 7610bfa3-a932-4ce4-95ff-e791c1c422e1.\n",
      "[2023-05-30 14:15:32Z] Submitting 1 runs, first five are: 3e9206d3:ce8e3808-7a9b-407d-82e4-3e04ae3e0a41\n",
      "[2023-05-30 14:16:09Z] Execution of experiment failed, update experiment status and cancel running nodes.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: yellow_wolf_bvfprd58p6\n",
      "Web View: https://ml.azure.com/runs/yellow_wolf_bvfprd58p6?wsid=/subscriptions/2213e8b1-dbc7-4d54-8aff-b5e315df5e5b/resourcegroups/1-6a7b0882-playground-sandbox/workspaces/MLOPS101\n"
     ]
    },
    {
     "ename": "JobException",
     "evalue": "Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /train_job. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"eastus2\",\n    \"location\": \"eastus2\",\n    \"time\": \"2023-05-30T14:16:09.911623Z\",\n    \"component_name\": \"\"\n} ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJobException\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mws\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/tracing/decorator.py:76\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:263\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_operations.py:645\u001b[0m, in \u001b[0;36mJobOperations.stream\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pipeline_child_job(job_object):\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineChildJobError(job_id\u001b[38;5;241m=\u001b[39mjob_object\u001b[38;5;241m.\u001b[39mid)\n\u001b[0;32m--> 645\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_logs_until_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runs_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datastore_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequests_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requests_pipeline\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_ops_helper.py:295\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[0;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[1;32m    293\u001b[0m         file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m JobException(\n\u001b[1;32m    296\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException : \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(json\u001b[38;5;241m.\u001b[39mdumps(error, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)),\n\u001b[1;32m    297\u001b[0m             target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mJOB,\n\u001b[1;32m    298\u001b[0m             no_personal_data_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException raised on failed job.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    299\u001b[0m             error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mSYSTEM_ERROR,\n\u001b[1;32m    300\u001b[0m         )\n\u001b[1;32m    302\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    303\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mJobException\u001b[0m: Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /train_job. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"eastus2\",\n    \"location\": \"eastus2\",\n    \"time\": \"2023-05-30T14:16:09.911623Z\",\n    \"component_name\": \"\"\n} "
     ]
    }
   ],
   "source": [
    "ws.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb3244-c64f-4e84-bbcc-2e45f1cbece6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
